% Text Mining Project Report
% ACL-style skeleton tailored for LiU text mining course

\documentclass[11pt]{article}

% ===== ACL style & basic packages =====
% 注意：编译时需要 acl 样式文件（acl.sty 等），
% 建议直接在 Overleaf 里使用官方 *ACL 模板，然后把本框架内容拷贝过去。
\usepackage[hyperref]{acl}
\usepackage{lmodern}
% Load microtype if available (allows compilation when package is missing)
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{float}
% \usepackage{microtype} % 可选；可尝试安装本地 microtype 来改善微排版

% Title
\title{Event-Driven Topic Shifts in The Guardian:\newline A Comparative Analysis of BERTopic and LDA around Queen Elizabeth II's Death}

% 多作者示例：用 \\ 换行分单位
\author{
  Liuxi Mei \\
  Department of Computer and Information Science \\
  Linköping University \\
  \texttt{liume102@student.liu.se}
}

\begin{document}
\maketitle
\sloppy

% ============ Abstract ============
\begin{abstract}
  We analyze how The Guardian's news agenda shifted around Queen Elizabeth II's death, using topic modelling on articles from 2020--2025. To isolate indirect effects, we exclude direct event coverage via buffered windows and keyword filtering. We compare Latent Dirichlet Allocation (LDA) and BERTopic, employing a novel semantic alignment strategy to track evolving themes across independent models. Results reveal a significant "agenda reset": diffuse reporting declined while specific narratives—notably Economic Policy and Political Leadership—consolidated. BERTopic provided superior granularity in capturing these shifts compared to LDA. Our findings demonstrate that major events drive substantial structural changes in media agendas, with established themes briefly displaced before returning with altered intensity.
\end{abstract}

% ============ 1 Introduction ============
\section{Introduction}
Major events can dramatically reshape media agendas \citep{mccombs1972agenda}, but measuring these shifts requires careful exclusion of event-dominated coverage. This study focuses on The Guardian's reporting before and after Queen Elizabeth II's death on September 8, 2022, a landmark event in UK history. We use topic modelling to quantify agenda changes, implementing buffered time windows and keyword-based filtering to remove direct event reports and obituaries. Our research questions are: (1) How do news topics shift around the event after excluding direct coverage? (2) How do LDA and BERTopic compare in capturing these shifts? (3) Are the observed changes robust to different modelling and filtering strategies?

Our contributions are: (1) A robust pipeline for event-exclusion in media agenda analysis; (2) Comparative evaluation of LDA and BERTopic on filtered Guardian news; (3) Statistical and temporal analysis confirming significant agenda shifts even after excluding direct event coverage.


% ============ 2 Background / Related Work ============
\section{Background and Related Work}
\begin{figure}[h!]
  \centering
  % BERTopic: 5 rows x 2 columns (pre | post)
  \begin{minipage}{\linewidth}
    \centering
    	\textbf{BERTopic — Pre (left) vs Post (right)}\\[2mm]
    \begin{tabular}{@{}cc@{}}
      \includegraphics[width=0.4\linewidth,trim=0 0 0 0,clip]{wordclouds/BERTopic_pre_topic_0.png} &
      \includegraphics[width=0.4\linewidth,trim=0 0 0 0,clip]{wordclouds/BERTopic_post_topic_0.png} \\
      \includegraphics[width=0.4\linewidth,trim=0 0 0 0,clip]{wordclouds/BERTopic_pre_topic_1.png} &
      \includegraphics[width=0.4\linewidth,trim=0 0 0 0,clip]{wordclouds/BERTopic_post_topic_1.png} \\
      \includegraphics[width=0.4\linewidth,trim=0 0 0 0,clip]{wordclouds/BERTopic_pre_topic_2.png} &
      \includegraphics[width=0.4\linewidth,trim=0 0 0 0,clip]{wordclouds/BERTopic_post_topic_2.png} \\
      \includegraphics[width=0.4\linewidth,trim=0 0 0 0,clip]{wordclouds/BERTopic_pre_topic_3.png} &
      \includegraphics[width=0.4\linewidth,trim=0 0 0 0,clip]{wordclouds/BERTopic_post_topic_3.png} \\
      \includegraphics[width=0.4\linewidth,trim=0 0 0 0,clip]{wordclouds/BERTopic_pre_topic_4.png} &
      \includegraphics[width=0.4\linewidth,trim=0 0 0 0,clip]{wordclouds/BERTopic_post_topic_4.png} \\
    \end{tabular}
  \end{minipage}

  \vspace{0.2cm}

  % LDA: 5 rows x 2 columns (pre | post)
  \begin{minipage}{\linewidth}
    \centering
    	\textbf{LDA — Pre (left) vs Post (right)}\\[2mm]
    \begin{tabular}{@{}cc@{}}
      \includegraphics[width=0.4\linewidth,trim=0 0 0 0,clip]{wordclouds/LDA_pre_topic_0.png} &
      \includegraphics[width=0.4\linewidth,trim=0 0 0 0,clip]{wordclouds/LDA_post_topic_0.png} \\
      \includegraphics[width=0.4\linewidth,trim=0 0 0 0,clip]{wordclouds/LDA_pre_topic_1.png} &
      \includegraphics[width=0.4\linewidth,trim=0 0 0 0,clip]{wordclouds/LDA_post_topic_1.png} \\
      \includegraphics[width=0.4\linewidth,trim=0 0 0 0,clip]{wordclouds/LDA_pre_topic_2.png} &
      \includegraphics[width=0.4\linewidth,trim=0 0 0 0,clip]{wordclouds/LDA_post_topic_2.png} \\
      \includegraphics[width=0.4\linewidth,trim=0 0 0 0,clip]{wordclouds/LDA_pre_topic_3.png} &
      \includegraphics[width=0.4\linewidth,trim=0 0 0 0,clip]{wordclouds/LDA_post_topic_3.png} \\
      \includegraphics[width=0.4\linewidth,trim=0 0 0 0,clip]{wordclouds/LDA_pre_topic_4.png} &
      \includegraphics[width=0.4\linewidth,trim=0 0 0 0,clip]{wordclouds/LDA_post_topic_4.png} \\
    \end{tabular}
  \end{minipage}

  \caption{Top-5 topic word clouds for BERTopic (top panel) and LDA (bottom panel). Each row shows pre-event (left sub-column) and post-event (right sub-column) wordclouds for the same topic index.}
  \label{fig:wordclouds-algo-prepost}
\end{figure}

\begin{table}[t]
  \centering
  \small
  \begin{tabular}{lrr}
    \toprule
    Metric & Pre-event & Post-event \\
    \midrule
    \#documents & 20,000 & 20,000 \\
    Avg. Tokens & 11.19 & 12.30 \\
    BERTopic (C\textsubscript{v}) & 0.3950 & 0.4455 \\
    LDA (C\textsubscript{UMass}) & -5.2734 & -4.6545 \\
    \bottomrule
  \end{tabular}
  \caption{Dataset statistics and model coherence metrics. Note: BERTopic uses $C_v$ (0--1), while LDA uses $C_{UMass}$ (typically negative).}
  \label{tab:summary-metrics}
\end{table}




Topic modelling aims to uncover latent semantic structure in document collections by representing each document as a mixture of topics and each topic as a distribution over words. LDA \citep{blei2003latent} remains one of the most widely used topic models due to its conceptual simplicity and efficient implementations. It has been applied extensively to news corpora to explore media agendas, political framing, and temporal trends \citep{dimaggio2013exploiting}. However, several studies have pointed out that LDA can produce incoherent topics, especially when documents are short or vocabulary is noisy, and that it may merge semantically distinct themes into a single topic \citep{newman2010automatic}.

To address some of these issues, recent work has explored neural and embedding-based topic models that leverage pre-trained language representations. BERTopic \citep{grootendorst2022bertopic} is a prominent example that combines transformer-based sentence embeddings, clustering, and class-based TF--IDF to derive interpretable topics. Prior work has shown that such approaches often yield higher topic coherence and more fine-grained distinctions than LDA, particularly on modern text genres such as news, reviews, and social media posts.

Dynamic topic models \citep{blei2006dynamic} extend static topic models by explicitly modelling how topic distributions change over time. While fully generative dynamic models can be complex to implement and infer, a simpler alternative is to train separate topic models on different time slices and analyse how topic prevalence and content vary across periods \citep{hall2008studying, hoyle2014computational}. This pragmatic strategy has been adopted in several studies of news and social media data to study temporal trends without requiring sophisticated temporal priors.

Our work is closest in spirit to research that compares classical probabilistic topic models with neural or embedding-based approaches on real-world corpora, and to studies that apply topic modelling to news archives for temporal analysis. Unlike most prior work, which typically focuses either on static topic quality or on dynamic trends within a single modelling framework, we combine a comparative evaluation of LDA and BERTopic with an explicit analysis of topic evolution over time on The Guardian dataset used in this study.


% ============ 3 Data ============
\section{Data}


We collected The Guardian's English news articles from 2020 to 2025 using a custom-developed Python scraper (\texttt{guardian\_news\_scraper.py}) that interfaces directly with The Guardian Open Platform API. This approach ensured high-fidelity data retrieval, capturing essential metadata including UTC-standardized publication dates, titles, and section information. The study focuses on the period around Queen Elizabeth II's death on September 8, 2022. To avoid event-dominated coverage, we exclude articles within a 30-day buffer before and after the event, and filter out texts containing keywords such as "obituary", "death", "tribute", "funeral", and direct references to the Queen or monarchy. Minimal preprocessing is applied: lowercasing, stopword removal, and basic cleaning. The final dataset enables robust analysis of agenda shifts while minimizing confounding from direct event reporting.

We constructed a balanced dataset by randomly sampling 20,000 articles for each period (Pre and Post). This equal sampling size was chosen to (1) prevent volume bias in topic prevalence comparisons, ensuring that observed shifts reflect genuine proportional changes rather than dataset imbalances, and (2) maintain computational feasibility for the resource-intensive BERTopic embeddings.

The pre-event period includes articles published before September 2022, while the post-event period includes those after. The 'filtered' designation indicates that direct event-related articles have been excluded. \#Documents denotes the number of articles in each split, and Avg. Tokens represents the average number of tokens per article after preprocessing.


% Dataset statistics (fill placeholders with Notebook outputs)


% Force table placement and scale to page width to avoid overlap with nearby floats
% Removed duplicate table - identical to the first dataset stats table







\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{topic_time_series.png}
  \caption{Weekly topic prevalence for top aligned BERTopic themes.}
  \label{fig:topic-timeseries}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{lda_combined_time_series.png}
  \caption{Weekly topic prevalence for top aligned LDA themes.}
  \label{fig:lda-combined-timeseries}
\end{figure}






% ============ 4 Method ============
\section{Method}
Our pipeline consists of: (1) Data cleaning and event-exclusion filtering; (2) Topic modelling with LDA and BERTopic; (3) Quantitative and qualitative comparison; (4) Statistical and temporal analysis.

\subsection{Event-Exclusion Filtering}
We exclude all articles within 30 days before and after the event date, and remove those containing event-related keywords. This ensures that the analysis focuses on indirect agenda shifts rather than direct event coverage.

\subsection{Topic Modelling}
We apply LDA (bag-of-words, tuned topic number) and BERTopic (transformer-based embeddings, clustering, class-based TF--IDF) to the filtered corpus. Importantly, we train separate models for the pre-event and post-event periods. This approach avoids enforcing a static vocabulary or topic structure, allowing the models to capture new themes that may emerge after the event.

\subsection{Temporal Topic Alignment}
Since independent models produce different topic IDs and structures for the pre- and post-event periods, we implement a semantic matching strategy to enable direct comparison.
\begin{itemize}
    \item \textbf{LDA Alignment:} We compute the cosine similarity between the topic-word distributions ($\beta$ vectors) of the Pre-LDA and Post-LDA models. For each prominent Pre-event topic, we identify the Post-event topic with the highest similarity score.
    \item \textbf{BERTopic Alignment:} We generate embedding vectors for each topic by aggregating the embeddings of their constituent documents (or using the c-TF-IDF vectors). We then calculate the cosine similarity between Pre-event and Post-event topic vectors to find the best semantic matches.
\end{itemize}
This alignment allows us to track how specific themes (e.g., "Economic Policy" or "Royal Family") evolved in prevalence and content, rather than comparing unrelated topic IDs.

\subsection{Comparison and Analysis}
We compare LDA and BERTopic by inspecting top words, representative articles, and topic distributions. Statistical tests (chi-square, t-test) and time series analysis are used to confirm the significance and robustness of observed agenda shifts.

\subsection{Statistical Verification}
We use standard test statistics to quantify changes in topic prevalence and evaluate topic quality.

\textbf{Topic Coherence Metrics:}
To assess the interpretability of the generated topics, we employ two distinct coherence measures suited to each model's architecture.
For LDA, we use $C_{UMass}$, which is based on document co-occurrence log-probabilities:
\[
  C_{UMass} = \sum_{i<j} \log \frac{D(w_i, w_j) + \epsilon}{D(w_i)}
\]
where $D(w_i, w_j)$ is the count of documents containing both words $w_i$ and $w_j$, and $D(w_i)$ is the count for word $w_i$ alone.
For BERTopic, we use $C_v$, which combines a sliding window co-occurrence with normalized pointwise mutual information (NPMI) and cosine similarity:
\[
  C_v = \sum_{i<j} \cos(\vec{v}_{w_i}, \vec{v}_{w_j})
\]
where $\vec{v}_{w}$ represents the context vector for word $w$ derived from NPMI statistics.

\textbf{Chi-square Test for Global Distributional Shift:}
We construct a contingency table where rows represent aligned topics and columns represent time periods (Pre vs Post). The Null Hypothesis ($H_0$) states that the topic distribution is independent of the time period, implying that the relative proportions of topics remain constant. The Pearson chi-square statistic is calculated as:
\[
  \chi^2 = \sum_{i,j} \frac{(O_{ij}-E_{ij})^2}{E_{ij}},
\]
where $O_{ij}$ is the observed document count for topic $i$ in period $j$, and $E_{ij}$ is the expected count under $H_0$. A significant result leads to the rejection of $H_0$, confirming a structural break in the media agenda.

\textbf{Z-test for Individual Topic Proportions:}
For specific topics of interest (e.g., those showing large absolute changes), we perform a Z-test for the difference of two proportions to determine if the change in prevalence is statistically significant. The test statistic is given by:
\[
  Z = \frac{\hat{p}_1 - \hat{p}_2}{\sqrt{\hat{p}(1-\hat{p})(\frac{1}{n_1} + \frac{1}{n_2})}},
\]
where $\hat{p}_1$ and $\hat{p}_2$ are the sample proportions of the topic in the pre- and post-event periods respectively, $\hat{p}$ is the pooled proportion, and $n_1, n_2$ are the sample sizes. This allows us to pinpoint exactly which narratives drove the global shift.


% ============ 5 Experiments ============
\section{Experiments}
We train LDA and BERTopic models on the filtered pre- and post-event corpora (Section~3). For BERTopic, we use \texttt{all-MiniLM-L6-v2} embeddings, UMAP, and HDBSCAN; for LDA, we use CountVectorizer and scikit-learn's implementation. Models were trained independently on the "no-event" subsets. Evaluation combines quantitative metrics (standard $C_v$ for BERTopic, $C_{UMass}$ for LDA) and qualitative inspection of top words and time series. Statistical tests include Pearson chi-square for distribution shifts and Z-tests for proportion differences.

% ============ 6 Analysis and Discussion ============
\section{Analysis and Discussion}
Our semantic alignment reveals clear agenda shifts.

\subsection{Aligned Topic Evolution}
Figures~\ref{fig:topic-timeseries} and \ref{fig:lda-combined-timeseries} show weekly prevalence for top pre-event topics and their post-event matches.
\textbf{BERTopic} (Figure~\ref{fig:topic-timeseries}) captures thematic continuity with distinct "dips" and "recoveries", validating the "agenda reset" hypothesis. Topics like "Political Leadership" show altered intensity post-event.
\textbf{LDA} (Figure~\ref{fig:lda-combined-timeseries}) shows coarser matches and flatter trends, as its bag-of-words nature blends distinct themes. BERTopic's sharper dynamics underscore its superior sensitivity to fine-grained shifts.

\subsection{Quantitative Shifts and Statistical Significance}
A Pearson chi-square test confirms a highly significant distribution change (\(\chi^2 = 491.618,\; p < 10^{-6}\)), indicating a structural break in the media agenda.
Z-tests pinpoint specific drivers: "Economic Policy" (Topic 0) surged from 1.59\% to 2.54\% ($Z \approx 6.58, p < 10^{-10}$), and "Political Leadership" (Topic 1) rose from 1.42\% to 2.09\% ($Z \approx 4.63, p < 10^{-5}$), confirming the consolidation of substantive narratives.

Table~\ref{tab:summary-metrics} summarizes key metrics. Coherence scores confirm topic quality: BERTopic achieves solid $C_v$ values (Pre: 0.3950, Post: 0.4455), with a +0.05 post-event increase suggesting a more focused agenda. LDA's low $C_{UMass}$ scores (-5.27 to -4.65) reflect its struggle with short news texts. Qualitatively, BERTopic produces more coherent topics, and time series confirm these shifts are sustained, not transient.




% ============ 7 Conclusion and Future Work ============
\section{Conclusion and Future Work}
We present a practical pipeline for measuring media agenda shifts around major events while minimizing the influence of direct event reports. Applied to The Guardian's coverage surrounding Queen Elizabeth II's death, our analysis reveals distinct differences in the news agenda. Despite excluding direct coverage, we observed a statistically significant "agenda reset": general "noise" or diffuse reporting decreased, while specific post-event narratives (e.g., Economic Policy, Political Leadership) saw consolidated growth (e.g., Topic 0 increasing from 318 to 508 documents). BERTopic proved superior to LDA in capturing these fine-grained shifts, showing how established themes were briefly displaced before returning with altered intensity. Future work should evaluate sensitivity to exclusion parameters and extend this analysis to multiple media outlets.


% ============ 8 Limitations (ACL 要求，简短即可) ============
\section{Limitations}
Our analysis is limited by the single-outlet scope (The Guardian) and the specific exclusion parameters (keyword list, 30-day buffer). While the buffer removes immediate event reports, indirect references likely remain. Additionally, our semantic alignment strategy assumes a 1-to-1 mapping between pre-event and post-event topics, which simplifies the reality where topics may split or merge (many-to-many). BERTopic also imposes higher computational costs compared to LDA. Future work should explore many-to-many alignment techniques and validate findings across broader news datasets.





% ============ References ============
\bibliography{references}
% 请在同一目录下创建 references.bib，并将所有引用文献放入其中；
% 使用 \citep 和 \citet 等引用命令符合 *ACL 风格。

\end{document}