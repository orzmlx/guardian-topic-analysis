{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Shifting Agendas: The Guardian and the Death of Queen Elizabeth II\n",
    "\n",
    "**Event-Driven Topic Shifts: A BERTopic Analysis of The Guardian Before and After the Death of Queen Elizabeth II**\n",
    "\n",
    "This notebook performs topic modeling, alignment, sentiment and entity analyses to quantify how The Guardian's news agenda shifted around the death of Queen Elizabeth II (2020â€“2025 coverage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. Setup & Imports\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# NLP & ML Libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Ensure NLTK resources are available\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "print(\"Libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. Utility Classes\n",
    "# ==========================================\n",
    "\n",
    "class TextAnalyzer:\n",
    "    \"\"\"Handles text preprocessing, loading, and dataset splitting.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        \"\"\"Basic text cleaning: lowercase, remove URLs, keep only letters.\"\"\"\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "        text = re.sub(r\"[^a-z\\s]\", \" \", text) # Keep only letters\n",
    "        text = re.sub(r\"\\s+\", \" \", text)      # Remove extra spaces\n",
    "        return text.strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_and_filter_data(filepath, start_date, end_date):\n",
    "        \"\"\"Load CSV, parse dates, and filter by the specified date range.\"\"\"\n",
    "        print(f\"Loading data from {filepath}...\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        # Convert date column to datetime objects (UTC)\n",
    "        df['date'] = pd.to_datetime(df['date'], utc=True, errors='coerce')\n",
    "        # Drop rows with invalid dates and sort\n",
    "        df = df.dropna(subset=['date']).sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        # Filter by date range\n",
    "        mask = (df['date'] >= pd.to_datetime(start_date, utc=True)) & \\\n",
    "               (df['date'] <= pd.to_datetime(end_date, utc=True))\n",
    "        df_filtered = df[mask].copy()\n",
    "        \n",
    "        # Apply text cleaning\n",
    "        df_filtered['clean_text'] = df_filtered['title'].apply(TextAnalyzer.clean_text)\n",
    "        return df_filtered\n",
    "\n",
    "    @staticmethod\n",
    "    def split_event_data(df, event_date, buffer_days=30, exclude_keywords=None):\n",
    "        \"\"\"Split data into Pre-Event and Post-Event sets, applying buffer zones and keyword exclusion.\"\"\"\n",
    "        event_dt = pd.to_datetime(event_date, utc=True)\n",
    "        \n",
    "        # Initial split based on event date\n",
    "        pre_df = df[df['date'] < event_dt].copy()\n",
    "        post_df = df[df['date'] >= event_dt].copy()\n",
    "        \n",
    "        # Apply temporal buffer (remove articles too close to the event)\n",
    "        pre_buffered = pre_df[pre_df['date'] <= (event_dt - pd.Timedelta(days=buffer_days))]\n",
    "        post_buffered = post_df[post_df['date'] >= (event_dt + pd.Timedelta(days=buffer_days))]\n",
    "        \n",
    "        # Filter out articles containing specific event-related keywords (No-Event subset)\n",
    "        if exclude_keywords:\n",
    "            pattern = '|'.join([re.escape(k) for k in exclude_keywords])\n",
    "            pre_clean = pre_buffered[~pre_buffered['clean_text'].str.contains(pattern, case=False, na=False)]\n",
    "            post_clean = post_buffered[~post_buffered['clean_text'].str.contains(pattern, case=False, na=False)]\n",
    "        else:\n",
    "            pre_clean, post_clean = pre_buffered, post_buffered\n",
    "            \n",
    "        return pre_clean, post_clean\n",
    "\n",
    "class TopicModeler:\n",
    "    \"\"\"Wrapper class for LDA and BERTopic model training and evaluation.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_lda(texts, n_topics=5):\n",
    "        \"\"\"Train Latent Dirichlet Allocation (LDA) model.\"\"\"\n",
    "        vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "        dtm = vectorizer.fit_transform(texts)\n",
    "        lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, n_jobs=-1)\n",
    "        lda.fit(dtm)\n",
    "        return lda, vectorizer\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_bertopic(texts, n_topics=5, min_topic_size=15):\n",
    "        \"\"\"Train BERTopic model using Sentence Transformers and UMAP/HDBSCAN.\"\"\"\n",
    "        embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        # Configure UMAP for dimensionality reduction\n",
    "        umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "        # Configure HDBSCAN for clustering\n",
    "        hdbscan_model = HDBSCAN(min_cluster_size=min_topic_size, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "        \n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            umap_model=umap_model,\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            nr_topics=n_topics,\n",
    "            verbose=True\n",
    "        )\n",
    "        topics, probs = topic_model.fit_transform(texts)\n",
    "        return topic_model, topics\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_cv_coherence(topics_words, texts):\n",
    "        \"\"\"Calculate C_v topic coherence score using Gensim.\"\"\"\n",
    "        dictionary = Dictionary(texts)\n",
    "        # Use processes=1 to avoid issues in some notebook environments\n",
    "        cm = CoherenceModel(topics=topics_words, texts=texts, dictionary=dictionary, coherence='c_v', processes=1)\n",
    "        return cm.get_coherence()\n",
    "\n",
    "print(\"Utility classes defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. Main Analysis Pipeline\n",
    "# ==========================================\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_PATH = 'guardian_news_std.csv'\n",
    "START_DATE = '2020-01-01'\n",
    "END_DATE = '2025-12-31'\n",
    "EVENT_DATE = '2022-09-08'  # Date of Queen Elizabeth II's death\n",
    "BUFFER_DAYS = 30\n",
    "EXCLUDE_KEYWORDS = [\n",
    "    'lord', 'queen', 'elizabeth', 'monarchy', 'royal', 'king charles', \n",
    "    'obituary', 'death', 'died', 'tribute', 'funeral'\n",
    "]\n",
    "\n",
    "# --- Step 1: Load and Filter Data ---\n",
    "full_df = TextAnalyzer.load_and_filter_data(DATA_PATH, START_DATE, END_DATE)\n",
    "\n",
    "# --- Step 2: Split Data (Pre-Event vs Post-Event) ---\n",
    "pre_df, post_df = TextAnalyzer.split_event_data(\n",
    "    full_df, EVENT_DATE, BUFFER_DAYS, EXCLUDE_KEYWORDS\n",
    ")\n",
    "\n",
    "# --- Step 3: Downsampling ---\n",
    "# Use a subset of data to balance classes and improve training speed\n",
    "SAMPLE_SIZE = 20000\n",
    "pre_sample = pre_df.sample(n=min(len(pre_df), SAMPLE_SIZE), random_state=42)\n",
    "post_sample = post_df.sample(n=min(len(post_df), SAMPLE_SIZE), random_state=42)\n",
    "\n",
    "print(f\"\\nData Summary (After removing articles with keywords: {', '.join(EXCLUDE_KEYWORDS[:3])}...):\")\n",
    "print(f\"Pre-Event Dataset:  {len(pre_sample)} documents\")\n",
    "print(f\"Post-Event Dataset: {len(post_sample)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. Model Training & Evaluation\n",
    "# ==========================================\n",
    "\n",
    "# Prepare text data for modeling\n",
    "pre_texts = pre_sample['clean_text'].tolist()\n",
    "post_texts = post_sample['clean_text'].tolist()\n",
    "# Tokenize for coherence calculation\n",
    "pre_tokens = [t.split() for t in pre_texts]  \n",
    "post_tokens = [t.split() for t in post_texts]\n",
    "\n",
    "# --- A. Train LDA Models ---\n",
    "print(\"\\n--- Training LDA Models (Baseline) ---\")\n",
    "lda_pre, vec_pre = TopicModeler.train_lda(pre_texts, n_topics=5)\n",
    "lda_post, vec_post = TopicModeler.train_lda(post_texts, n_topics=5)\n",
    "\n",
    "# Helper to extract top words from LDA\n",
    "def get_lda_top_words(model, feature_names, n_top_words=10):\n",
    "    return [[feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]] \n",
    "            for topic in model.components_]\n",
    "\n",
    "lda_pre_topics = get_lda_top_words(lda_pre, vec_pre.get_feature_names_out())\n",
    "lda_post_topics = get_lda_top_words(lda_post, vec_post.get_feature_names_out())\n",
    "\n",
    "# Compute LDA Coherence\n",
    "print(\"Calculating LDA Coherence (C_v)...\")\n",
    "coh_lda_pre = TopicModeler.compute_cv_coherence(lda_pre_topics, pre_tokens)\n",
    "coh_lda_post = TopicModeler.compute_cv_coherence(lda_post_topics, post_tokens)\n",
    "\n",
    "# --- B. Train BERTopic Models ---\n",
    "print(\"\\n--- Training BERTopic Models (Advanced) ---\")\n",
    "bert_pre, _ = TopicModeler.train_bertopic(pre_texts, n_topics=5)\n",
    "bert_post, _ = TopicModeler.train_bertopic(post_texts, n_topics=5)\n",
    "\n",
    "# Helper to extract top words from BERTopic (skipping outlier topic -1)\n",
    "def get_bert_top_words(model):\n",
    "    topics = []\n",
    "    # Iterate through topics, typically starting from 0\n",
    "    for i in range(len(model.get_topic_info()) - 1):\n",
    "        if i in model.get_topics():\n",
    "            topics.append([word for word, _ in model.get_topic(i)][:10])\n",
    "    return topics\n",
    "\n",
    "bert_pre_topics = get_bert_top_words(bert_pre)\n",
    "bert_post_topics = get_bert_top_words(bert_post)\n",
    "\n",
    "# Compute BERTopic Coherence\n",
    "print(\"Calculating BERTopic Coherence (C_v)...\")\n",
    "coh_bert_pre = TopicModeler.compute_cv_coherence(bert_pre_topics, pre_tokens)\n",
    "coh_bert_post = TopicModeler.compute_cv_coherence(bert_post_topics, post_tokens)\n",
    "\n",
    "# --- Display Summary ---\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['LDA', 'LDA', 'BERTopic', 'BERTopic'],\n",
    "    'Period': ['Pre-Event', 'Post-Event', 'Pre-Event', 'Post-Event'],\n",
    "    'Coherence (Cv)': [coh_lda_pre, coh_lda_post, coh_bert_pre, coh_bert_post]\n",
    "})\n",
    "print(\"\\nModel Evaluation Summary:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. Visualization\n",
    "# ==========================================\n",
    "\n",
    "def plot_wordclouds(topics_words, title):\n",
    "    \"\"\"Generate and display word clouds for the identified topics.\"\"\"\n",
    "    from wordcloud import WordCloud\n",
    "    \n",
    "    n_topics = len(topics_words)\n",
    "    if n_topics == 0:\n",
    "        print(f\"No topics found for {title}\")\n",
    "        return\n",
    "        \n",
    "    fig, axes = plt.subplots(1, n_topics, figsize=(20, 4))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "    for i, words in enumerate(topics_words):\n",
    "        # Create word cloud from list of words\n",
    "        wc = WordCloud(background_color='white', width=400, height=300).generate(' '.join(words))\n",
    "        \n",
    "        # Handle single subplot case\n",
    "        ax = axes[i] if n_topics > 1 else axes\n",
    "        ax.imshow(wc, interpolation='bilinear')\n",
    "        ax.set_title(f'Topic {i}')\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualizing LDA Topics...\")\n",
    "plot_wordclouds(lda_pre_topics, \"LDA Pre-Event Topics\")\n",
    "plot_wordclouds(lda_post_topics, \"LDA Post-Event Topics\")\n",
    "\n",
    "print(\"Visualizing BERTopic Topics...\")\n",
    "plot_wordclouds(bert_pre_topics, \"BERTopic Pre-Event Topics\")\n",
    "plot_wordclouds(bert_post_topics, \"BERTopic Post-Event Topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 6. Save Outputs\n",
    "# ==========================================\n",
    "\n",
    "output_dir = 'results_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save metrics to CSV\n",
    "results_df.to_csv(os.path.join(output_dir, 'coherence_metrics.csv'), index=False)\n",
    "\n",
    "# Save detailed topic keywords to text file\n",
    "with open(os.path.join(output_dir, 'topic_keywords.txt'), 'w') as f:\n",
    "    f.write(\"LDA Pre-Event Topics:\\n\" + str(lda_pre_topics) + \"\\n\\n\")\n",
    "    f.write(\"LDA Post-Event Topics:\\n\" + str(lda_post_topics) + \"\\n\\n\")\n",
    "    f.write(\"BERTopic Pre-Event Topics:\\n\" + str(bert_pre_topics) + \"\\n\\n\")\n",
    "    f.write(\"BERTopic Post-Event Topics:\\n\" + str(bert_post_topics) + \"\\n\\n\")\n",
    "\n",
    "print(f\"All results have been saved to the directory: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}